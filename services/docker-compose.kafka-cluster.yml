version: '3.4'
services:
  cassandra:
    image: cassandra:latest
    ports:
      - '9042:9042'
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    depends_on:
      - zookeeper
      - kafka-broker
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
  kafka-broker:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka-broker:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
  connect:
    image: confluentinc/kafka-connect-datagen:latest
    build:
      context: .
      dockerfile: Docker.Kafka.Connect.Cassandra.Dockerfile
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper
      - kafka-broker
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka-broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # Assumes image is based on confluentinc/kafka-connect-datagen:latest which is pulling 5.1.1 Connect image
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-5.2.1.jar
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR






































# ---
# version: '2'
# services:
#   zookeeper:
#     image: confluentinc/cp-zookeeper:5.2.1
#     hostname: zookeeper
#     container_name: zookeeper
#     ports:
#       - "2181:2181"
#     environment:
#       ZOOKEEPER_CLIENT_PORT: 2181
#       ZOOKEEPER_TICK_TIME: 2000

#   broker:
#     image: confluentinc/cp-enterprise-kafka:5.2.1
#     hostname: broker
#     container_name: broker
#     depends_on:
#       - zookeeper
#     ports:
#       - "29092:29092"
#       - "9092:9092"
#     environment:
#       KAFKA_BROKER_ID: 1
#       KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
#       KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
#       KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
#       KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
#       KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#       KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
#       CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
#       CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
#       CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
#       CONFLUENT_METRICS_ENABLE: 'true'
#       CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

#   schema-registry:
#     image: confluentinc/cp-schema-registry:5.2.1
#     hostname: schema-registry
#     container_name: schema-registry
#     depends_on:
#       - zookeeper
#       - broker
#     ports:
#       - "8081:8081"
#     environment:
#       SCHEMA_REGISTRY_HOST_NAME: schema-registry
#       SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'

#   connect:
#     image: confluentinc/kafka-connect-datagen:latest
#     build:
#       context: .
#       dockerfile: Dockerfile
#     hostname: connect
#     container_name: connect
#     depends_on:
#       - zookeeper
#       - broker
#       - schema-registry
#     ports:
#       - "8083:8083"
#     environment:
#       CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
#       CONNECT_REST_ADVERTISED_HOST_NAME: connect
#       CONNECT_REST_PORT: 8083
#       CONNECT_GROUP_ID: compose-connect-group
#       CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
#       CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
#       CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
#       CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
#       CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
#       CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
#       CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
#       CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
#       CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
#       CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
#       CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
#       CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
#       CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
#       # Assumes image is based on confluentinc/kafka-connect-datagen:latest which is pulling 5.1.1 Connect image
#       CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-5.2.1.jar
#       CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
#       CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
#       CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
#       CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

#   control-center:
#     image: confluentinc/cp-enterprise-control-center:5.2.1
#     hostname: control-center
#     container_name: control-center
#     depends_on:
#       - zookeeper
#       - broker
#       - schema-registry
#       - connect
#       - ksql-server
#     ports:
#       - "9021:9021"
#     environment:
#       CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'
#       CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:2181'
#       CONTROL_CENTER_CONNECT_CLUSTER: 'connect:8083'
#       CONTROL_CENTER_KSQL_URL: "http://ksql-server:8088"
#       CONTROL_CENTER_KSQL_ADVERTISED_URL: "http://localhost:8088"
#       CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
#       CONTROL_CENTER_REPLICATION_FACTOR: 1
#       CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
#       CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
#       CONFLUENT_METRICS_TOPIC_REPLICATION: 1
#       PORT: 9021

#   ksql-server:
#     image: confluentinc/cp-ksql-server:5.2.1
#     hostname: ksql-server
#     container_name: ksql-server
#     depends_on:
#       - broker
#       - connect
#     ports:
#       - "8088:8088"
#     environment:
#       KSQL_CONFIG_DIR: "/etc/ksql"
#       KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties"
#       KSQL_BOOTSTRAP_SERVERS: "broker:29092"
#       KSQL_HOST_NAME: ksql-server
#       KSQL_APPLICATION_ID: "cp-all-in-one"
#       KSQL_LISTENERS: "http://0.0.0.0:8088"
#       KSQL_CACHE_MAX_BYTES_BUFFERING: 0
#       KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
#       KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
#       KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"

#   ksql-cli:
#     image: confluentinc/cp-ksql-cli:5.2.1
#     container_name: ksql-cli
#     depends_on:
#       - broker
#       - connect
#       - ksql-server
#     entrypoint: /bin/sh
#     tty: true

#   ksql-datagen:
#     # Downrev ksql-examples to 5.1.2 due to DEVX-798 (work around issues in 5.2.0)
#     image: confluentinc/ksql-examples:5.1.2
#     hostname: ksql-datagen
#     container_name: ksql-datagen
#     depends_on:
#       - ksql-server
#       - broker
#       - schema-registry
#       - connect
#     command: "bash -c 'echo Waiting for Kafka to be ready... && \
#                        cub kafka-ready -b broker:29092 1 40 && \
#                        echo Waiting for Confluent Schema Registry to be ready... && \
#                        cub sr-ready schema-registry 8081 40 && \
#                        echo Waiting a few seconds for topic creation to finish... && \
#                        sleep 11 && \
#                        tail -f /dev/null'"
#     environment:
#       KSQL_CONFIG_DIR: "/etc/ksql"
#       KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties"
#       STREAMS_BOOTSTRAP_SERVERS: broker:29092
#       STREAMS_SCHEMA_REGISTRY_HOST: schema-registry
#       STREAMS_SCHEMA_REGISTRY_PORT: 8081

#   rest-proxy:
#     image: confluentinc/cp-kafka-rest:5.2.1
#     depends_on:
#       - zookeeper
#       - broker
#       - schema-registry
#     ports:
#       - 8082:8082
#     hostname: rest-proxy
#     container_name: rest-proxy
#     environment:
#       KAFKA_REST_HOST_NAME: rest-proxy
#       KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092'
#       KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
#       KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'

      
#   cassandra-seed:
#     container_name: cassandra-seed-node
#     image: cassandra:latest
#     ports:
#       - "9042:9042"   # Native transport
#       - "7199:7199"   # JMX
#       - "9160:9160"   # Thrift clients

#   cassandra-node-1:
#     image: cassandra:latest
#     command: /bin/bash -c "echo 'Waiting for seed node' && sleep 30 && /docker-entrypoint.sh cassandra -f"
#     environment:
#       - "CASSANDRA_SEEDS=cassandra-seed-node"
#     depends_on:
#       - "cassandra-seed"

#   # you cannot have multiple nodes join the cluster at the same time when
#   # cassandra.consistent.rangemovement is true so we further delay it to give it time to stabilize
#   cassandra-node-2:
#     image: cassandra:latest
#     command: /bin/bash -c "echo 'Waiting for seed node' && sleep 80 && /docker-entrypoint.sh cassandra -f"
#     environment:
#       - "CASSANDRA_SEEDS=cassandra-seed-node"
#     depends_on:
#       - "cassandra-seed"            






# CREATE TABLE test_cas.orders ( ip text PRIMARY KEY, message text);
# INSERT INTO test_cas.orders (ip,message) VALUES ('123','messageasd');

# SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'testkeyspace';



# curl -X POST \
#   -H "Content-Type: application/json" \
#   --data '{
#   "name" : "cassandratopic2",
#   "config" : {
# 		"name": "cassandratopic2",
# 		"connector.class": "io.confluent.connect.cassandra.CassandraSinkConnector",
# 		"cassandra.contact.points": "cassandra",
# 		"cassandra.keyspace.replication.factor":1,
# 		"topics": "jsontopic2",
# 		"cassandra.keyspace": "test_keyspace",
# 		"value.converter.schemas.enable": "false",
# 		"value.converter": "org.apache.kafka.connect.json.JsonConverter",
# 		"transforms": "insertKey, sanitizeTopic",
#         "transforms.insertKey.type": "org.apache.kafka.connect.transforms.ValueToKey",
#         "transforms.insertKey.fields": "ip",
#         "transforms.sanitizeTopic.type": "org.apache.kafka.connect.transforms.RegexRouter",
#         "transforms.sanitizeTopic.regex": "jsontopic2",
#         "transforms.sanitizeTopic.replacement": "jsontopic_table"
# }}
#   ' \
#   http://localhost:8083/connectors 





# using Avro;
# using Avro.Generic;
# using Confluent.Kafka;
# using Confluent.Kafka.Serialization;
# using Newtonsoft.Json;
# using System;
# using System.Collections.Generic;
# using System.IO;
# using System.Linq;
# using System.Text;

# namespace KafkaProducer
# {
#     class onur
#     {
#         public string ip { get; set; }
#         public string message { get; set; }
#         //public string date { get; set; }
#     }

#     class Program
#     {
#         private static Random random = new Random();
#         public static string RandomString(int length)
#         {
#             const string chars = "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
#             return new string(Enumerable.Repeat(chars, length)
#               .Select(s => s[random.Next(s.Length)]).ToArray());
#         }

#         static void Main(string[] args)
#         {
#             var config = new Dictionary<string, object>
#             {
#                 { "bootstrap.servers", "localhost:9092" }
#             };

#             //using (var producer = new Producer<Null, string>(config, null, new StringSerializer(Encoding.UTF8)))
#             //{
#             //    string text = null;

#             //    while (text != "quit")
#             //    {
#             //        onur a = new onur { ip = RandomString(10), message = RandomString(10) };

#             //        Console.Write("Add Message: ");
#             //        text = Console.ReadLine();
#             //        producer.ProduceAsync("jsontopic", null, JsonConvert.SerializeObject(a));
#             //    }
#             //    producer.Flush(100);
#             //}



#             using (var producer = new Producer<string, string>(config, new StringSerializer(Encoding.UTF8), new StringSerializer(Encoding.UTF8)))
#             {
#                 string text = null;

#                 while (text != "quit")
#                 {
#                     //onur a = new onur { ip = 5, message = 8 };
#                     onur a = new onur { ip = RandomString(10), message = RandomString(10) };

#                     Console.Write("Add Message: ");
#                     text = Console.ReadLine();
#                     var d = JsonConvert.SerializeObject(new { ip = a.ip });

#                     producer.ProduceAsync("jsontopicwithkey", JsonConvert.SerializeObject(new { ip = a.ip }), JsonConvert.SerializeObject(a));
#                 }
#                 producer.Flush(100);
#             }



#             //var config = new Dictionary<string, object>
#             //{
#             //    { "bootstrap.servers", "localhost:9092" },
#             //    { "schema.registry.url", "localhost:8081" }
#             //};

#             //using (var producer = new Producer<string, GenericRecord>(config, new StringSerializer(Encoding.UTF8), new AvroSerializer<GenericRecord>()))
#             //{
#             //    //var logLevelSchema = (EnumSchema)Schema.Parse(
#             //    //File.ReadAllText("LogLevel.asvc"));

#             //    var logMessageSchema = (RecordSchema)Schema
#             //    .Parse(File.ReadAllText("LogMessage.V2.asvc"));

#             //    var record = new GenericRecord(logMessageSchema);
#             //    record.Add("ip", "127.0.0.1");
#             //    record.Add("message", "log message2");
#             //    producer.ProduceAsync("orders-topic", "1", record)
#             //        .ContinueWith(dr => Console.WriteLine(dr.Result.Error
#             //            ? $"error producing message: {dr.Result.Error.Reason}"
#             //            : $"produced to: {dr.Result.TopicPartitionOffset}"));

#             //    producer.Flush(TimeSpan.FromSeconds(30));
#             //}
#         }
#     }
# }
